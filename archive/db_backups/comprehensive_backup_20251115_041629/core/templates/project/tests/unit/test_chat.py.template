"""
Unit tests for chat functionality.

This module tests the chat endpoints and AI service functionality.
"""

import pytest
from unittest.mock import AsyncMock, patch
from fastapi.testclient import TestClient
from httpx import AsyncClient

from src.models.chat import ChatRequest
from src.services.ai_service import AIService


class TestChatEndpoints:
    """Test cases for chat endpoints."""

    def test_chat_endpoint_success(self, client: TestClient, sample_chat_request):
        """Test successful chat request."""
        with patch.object(AIService, 'chat_completion') as mock_chat:
            mock_response = {
                "message": "Test response",
                "model": "gpt-3.5-turbo",
                "tokens_used": 10,
                "cost": 0.000015,
                "temperature": 0.7,
                "response_time_ms": 500,
            }
            mock_chat.return_value = mock_response

            response = client.post("/api/v1/chat/", json=sample_chat_request)
            assert response.status_code == 200

            data = response.json()
            assert data["message"] == "Test response"
            assert data["model"] == "gpt-3.5-turbo"
            assert data["tokens_used"] == 10

    def test_chat_endpoint_empty_message(self, client: TestClient):
        """Test chat request with empty message."""
        request_data = {
            "message": "",
            "model": "gpt-3.5-turbo",
            "temperature": 0.7,
            "max_tokens": 100,
        }

        response = client.post("/api/v1/chat/", json=request_data)
        assert response.status_code == 422  # Validation error

    def test_chat_endpoint_too_long_message(self, client: TestClient):
        """Test chat request with too long message."""
        request_data = {
            "message": "a" * 10001,  # Over limit
            "model": "gpt-3.5-turbo",
            "temperature": 0.7,
            "max_tokens": 100,
        }

        response = client.post("/api/v1/chat/", json=request_data)
        assert response.status_code == 422  # Validation error

    def test_chat_endpoint_invalid_temperature(self, client: TestClient, sample_chat_request):
        """Test chat request with invalid temperature."""
        sample_chat_request["temperature"] = 3.0  # Over limit

        response = client.post("/api/v1/chat/", json=sample_chat_request)
        assert response.status_code == 422  # Validation error

    def test_list_models_endpoint(self, client: TestClient):
        """Test list models endpoint."""
        response = client.get("/api/v1/chat/models")
        assert response.status_code == 200

        data = response.json()
        assert "available" in data
        assert "default" in data
        assert len(data["available"]) > 0

    def test_get_chat_history_endpoint(self, client: TestClient):
        """Test get chat history endpoint."""
        response = client.get("/api/v1/chat/history")
        assert response.status_code == 200

        data = response.json()
        assert "history" in data
        assert "total_messages" in data


class TestAIService:
    """Test cases for AI service."""

    @pytest.fixture
    def ai_service(self):
        """Create AI service instance for testing."""
        # Mock the API key for testing
        with patch('src.services.ai_service.settings.OPENAI_API_KEY', 'test-key'):
            return AIService()

    @pytest.mark.asyncio
    async def test_chat_completion_success(self, ai_service, mock_openai_response):
        """Test successful chat completion."""
        with patch.object(ai_service.client, 'chat') as mock_chat:
            mock_chat.completions.create = AsyncMock(return_value=mock_openai_response)

            response = await ai_service.chat_completion(
                message="Hello",
                model="gpt-3.5-turbo",
                temperature=0.7,
                max_tokens=100,
            )

            assert response.message == "Test response"
            assert response.model == "gpt-3.5-turbo"
            assert response.tokens_used == 10
            assert response.cost > 0

    @pytest.mark.asyncio
    async def test_rate_limit_check(self, ai_service):
        """Test rate limiting functionality."""
        # Set rate limit to 1 for testing
        with patch('src.services.ai_service.settings.RATE_LIMIT_PER_MINUTE', 1):
            # First request should pass
            assert ai_service._check_rate_limit() == True

            # Second request should fail
            assert ai_service._check_rate_limit() == False

    def test_cost_calculation(self, ai_service):
        """Test cost calculation."""
        cost = ai_service._calculate_cost(1000, "gpt-3.5-turbo")
        assert cost == 0.0015  # 1000 * 0.0000015

        cost = ai_service._calculate_cost(1000, "gpt-4")
        assert cost == 0.03  # 1000 * 0.00003

    def test_cost_limit_check(self, ai_service):
        """Test cost limit checking."""
        with patch('src.services.ai_service.settings.COST_BUDGET_PER_DAY', 1.0):
            # Small cost should pass
            assert ai_service._check_cost_limit(0.5) == True

            # Large cost should fail
            assert ai_service._check_cost_limit(2.0) == False

    def test_usage_stats(self, ai_service):
        """Test usage statistics."""
        stats = ai_service.get_usage_stats()

        assert "daily_cost" in stats
        assert "daily_budget" in stats
        assert "requests_this_minute" in stats
        assert "rate_limit_per_minute" in stats
